---
title: 'Reproducible Research: Peer Assessment 2'
author: "Katya Demidova"
date: "13 November 2015"
output: html_document
---
Your document should have a title that briefly summarizes your data analysis

Synopsis: Immediately after the title, there should be a synopsis which describes and summarizes your analysis in at most 10 complete sentences.

The analysis document must have at least one figure containing a plot. Your analyis must have no more than three figures. Figures may have multiple plots in them (i.e. panel plots), but there cannot be more than three figures total.

You must show all your code for the work in your analysis document. This may make the document a bit verbose, but that is okay. In general, you should ensure that echo = TRUE for every code chunk (this is the default setting in knitr).

## Data

The data for this assignment (comma-separated-value file compressed via the bzip2 algorithm):

* [Storm Data][1] [47Mb]

Documentation of the database:

* National Weather Service [Storm Data Documentation][2]
* National Climatic Data Center Storm Events [FAQ][3]

[1]: https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2
[2]: https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2Fpd01016005curr.pdf
[3]: https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2FNCDC%20Storm%20Events-FAQ%20Page.pdf

The events in the database start in the year 1950 and end in November 2011. In the earlier years of the database there are generally fewer events recorded, most likely due to a lack of good records. More recent years should be considered more complete.

## Questions

1. Across the United States, which types of events (as indicated in the `EVTYPE` variable) are most harmful with respect to population health?

2. Across the United States, which types of events have the greatest economic consequences?

## Data Processing

Data Processing describes (in words and code) how the data were loaded into R and processed for analysis. In particular, your analysis must start from the raw CSV file containing the data. You cannot do any preprocessing outside the document. If preprocessing is time-consuming you may consider using the cache = TRUE option for certain code chunks.

```{r, results='hide'}
# Load libraries
library(data.table)
library(dplyr)
library(tidyr)
```

### Import the data

The CSV file is very large (561.6 MB), so to load the data for the first time we use `fread()` method. In R Documentation `fread {data.table}` is described as similar to `read.table` but faster and more convenient.

After reading in the original CSV file, we save the dataset as an RDS file so we could restore the dataset via `readRDS()`. `readRDS()` is considered faster than other `read` methods. 

*These tips were found at the Coursera Discussion Forums. Many thanks to my coursemates*

```{r, cache=TRUE, results='hide'}
# Import the data
storm <- fread("StormData.csv", na.strings = c("NA", ""))

saveRDS(storm, "StormData.rds")
storm <- readRDS("StormData.rds")

#============================================================== Create a subset ???????

```

Quick overview of the internal structure of the dataset, its first and last rows:

```{r, cache=TRUE}
str(storm)
head(storm)
tail(storm)
```

### Explain the variables
There are 37 columns in this dataset. Their names are not obvious and need further explanation.

```{r, cache=TRUE, results='hide'}

#============================================================== HOW ???????

```

### Process the data into a format suitable for further analysis
```{r, cache=TRUE}
```
```{r, cache=TRUE}
```
```{r, cache=TRUE}
```
## Results
Results
